{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "261edaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import                os\n",
    "import                re\n",
    "import              glob\n",
    "import xarray      as xr\n",
    "import numpy       as np\n",
    "import pandas      as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d849d49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- SETTINGS TO FIX ----------\n",
    "dir_path = \"/scratch/gpfs/APORPORA/skanand/SoilRespiration/RS/monthly720_360\"  # folder with .nc files\n",
    "#dir_path = \"/scratch/gpfs/APORPORA/skanand/SoilRespiration/RS/4320_2160/monthly\"  # folder with .nc files\n",
    "\n",
    "neonsites_csv = \"./Neonsites_lat_long.csv\"   # NEON sites CSV in notebook folder\n",
    "lat_col = \"lat\"                              # column name for latitude in neonsites.csv\n",
    "lon_col = \"lon\"                              # column name for longitude in neonsites.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e38f089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sites\n",
    "sites_df = pd.read_csv(neonsites_csv)\n",
    "sites_df = sites_df.rename(columns={c: c.strip() for c in sites_df.columns})\n",
    "if \"lat\" not in sites_df.columns or \"lon\" not in sites_df.columns:\n",
    "    raise ValueError(\"neonsites.csv must contain 'lat' and 'lon' columns (decimal degrees).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f27a7cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list files\n",
    "files = sorted(glob.glob(os.path.join(dir_path, \"*.nc\")))\n",
    "if not files:\n",
    "    raise RuntimeError(f\"No .nc files found in {dir_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cfecfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare file column names and also keep mapping file->(var,year)\n",
    "file_cols = []\n",
    "file_info = {}  # fpath -> colname\n",
    "for f in files:\n",
    "    stem = Path(f).stem\n",
    "    # parse variable id and year\n",
    "    var_match = re.match(r\"(GPP|NEE|TER)\", stem, flags=re.IGNORECASE)\n",
    "    var_id = var_match.group(1).upper() if var_match else \"VAR\"\n",
    "    year_match = re.search(r\"\\.(\\d{4})$\", stem)\n",
    "    year = year_match.group(1) if year_match else \"0000\"\n",
    "    colname = f\"{var_id}_{year}\"\n",
    "    # ensure unique if name collision\n",
    "    i = 1\n",
    "    base = colname\n",
    "    while colname in file_cols:\n",
    "        colname = f\"{base}_{i}\"\n",
    "        i += 1\n",
    "    file_cols.append(colname)\n",
    "    file_info[f] = colname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b0da168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing site ABBY (US-xAB) at (45.76244,-122.33032)\n",
      "Processing site BART (US-xBR) at (44.06389,-71.28737)\n",
      "Processing site BLAN (US-xBL) at (39.0337,-78.04179)\n",
      "Processing site CLBJ (US-xCL) at (33.40123,-97.57)\n",
      "Processing site CPER (US-xCP) at (40.81554,-104.74559)\n",
      "Processing site DCFS (US-xDC) at (47.16165,-99.10656)\n",
      "Processing site DELA (US-xDL) at (32.54173,-87.80388)\n",
      "Processing site HARV (US-xHA) at (42.53691,-72.17265)\n",
      "Processing site JERC (US-xJE) at (31.19484,-84.46862)\n",
      "Processing site JORN (US-xJR) at (32.59069,-106.84254)\n",
      "Processing site KONA (US-xKA) at (39.11045,-96.61293)\n",
      "Processing site KONZ (US-xKZ) at (39.10077,-96.56307)\n",
      "Processing site LAJA (US-XLA) at (18.02126,-67.07689)\n",
      "Processing site MOAB (US-xMB) at (38.24828,-109.38827)\n",
      "Processing site NIWO (US-xNW) at (40.05425,-105.58237)\n",
      "Processing site NOGP (US-xNG) at (46.76972,-100.91535)\n",
      "Processing site ONAQ (US-xNQ) at (40.1776,-112.45245)\n",
      "Processing site ORNL (US-xRN) at (35.96413,-84.28259)\n",
      "Processing site OSBS (US-xSB) at (29.68928,-81.99343)\n",
      "Processing site RMNP (US-xRM) at (40.2759,-105.54596)\n",
      "Processing site SCBI (US-xSC) at (38.89292,-78.13949)\n",
      "Processing site SERC (US-xSE) at (38.89013,-76.56001)\n",
      "Processing site SRER (US-xSR) at (31.91068,-110.83549)\n",
      "Processing site STER (US-xSL) at (40.46189,-103.02929)\n",
      "Processing site TALL (US-xTA) at (32.95047,-87.39326)\n",
      "Processing site TREE (US-xTR) at (45.49369,-89.58571)\n",
      "Processing site UKFS (US-xUK) at (39.04043,-95.19215)\n",
      "Processing site UNDE (US-xUN) at (46.23391,-89.53725)\n",
      "Processing site WOOD (US-xWD) at (47.1282,-99.24133)\n",
      "Saved results to: ./Neondata_Fluxcom_720_360.csv\n",
      "Output shape (rows x cols): (348, 49)\n"
     ]
    }
   ],
   "source": [
    "# build a list of rows: each row = one month for one site\n",
    "rows = []\n",
    "\n",
    "# Iterate sites\n",
    "for sidx, srow in sites_df.iterrows():\n",
    "    NEONsite = srow.get(\"NEONsite\", \"\") if \"NEONsite\" in srow else \"\"\n",
    "    FluxnetSite = srow.get(\"FluxnetSite\", \"\") if \"FluxnetSite\" in srow else \"\"\n",
    "    location = srow.get(\"location\", \"\") if \"location\" in srow else \"\"\n",
    "    lat0 = float(srow[lat_col])\n",
    "    lon0 = float(srow[lon_col])\n",
    "    print(f\"Processing site {NEONsite} ({FluxnetSite}) at ({lat0},{lon0})\")\n",
    "\n",
    "    # for this site collect a dict of column -> 12-month array\n",
    "    site_vals = {}\n",
    "    for fpath, colname in file_info.items():\n",
    "        ds = None\n",
    "        try:\n",
    "            ds = xr.open_dataset(fpath)\n",
    "            # pick variable inside dataset that matches var_id, else first var\n",
    "            var_id = colname.split(\"_\")[0]\n",
    "            candidates = [v for v in ds.data_vars if var_id.lower() in v.lower()]\n",
    "            varname = candidates[0] if candidates else list(ds.data_vars)[0]\n",
    "            da = ds[varname]\n",
    "\n",
    "            # find lat/lon coord names\n",
    "            lat_name = [c for c in da.coords if \"lat\" in c.lower()][0]\n",
    "            lon_name = [c for c in da.coords if \"lon\" in c.lower()][0]\n",
    "\n",
    "            # linear interpolation (assumes scipy available)\n",
    "            interp_da = da.interp({lat_name: lat0, lon_name: lon0})\n",
    "            vals = np.asarray(interp_da.values).squeeze()\n",
    "\n",
    "            # ensure length 12 (pad or truncate)\n",
    "            if vals.size < 12:\n",
    "                padded = np.full(12, np.nan, dtype=float)\n",
    "                padded[: vals.size] = vals\n",
    "                vals = padded\n",
    "            else:\n",
    "                vals = vals[:12]\n",
    "\n",
    "            site_vals[colname] = vals\n",
    "\n",
    "        except Exception as e:\n",
    "            # Log and skip this file (store NaNs so downstream indexing works)\n",
    "            print(f\"WARNING: Skipping file due to error: {Path(fpath).name}\")\n",
    "            print(f\"         Error: {e!r}\")\n",
    "            site_vals[colname] = np.full(12, np.nan, dtype=float)\n",
    "        finally:\n",
    "            if ds is not None:\n",
    "                try:\n",
    "                    ds.close()\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "    # append 12 rows (months) for this site\n",
    "    for month in range(1, 13):\n",
    "        row = {\n",
    "            \"NEONsite\": NEONsite,\n",
    "            \"FluxnetSite\": FluxnetSite,\n",
    "            \"location\": location,\n",
    "            \"month\": month\n",
    "        }\n",
    "        # add each file column value for this month (month-1 index)\n",
    "        for col in file_cols:\n",
    "            row[col] = site_vals[col][month - 1]\n",
    "        rows.append(row)\n",
    "\n",
    "# build DataFrame and save to CSV in same folder as .nc files\n",
    "out_df = pd.DataFrame(rows)\n",
    "\n",
    "out_path = \"./Neondata_Fluxcom_720_360.csv\"\n",
    "#out_path = \"./Neondata_Fluxcom_4320_2160.csv\"\n",
    "\n",
    "out_df.to_csv(out_path, index=False)\n",
    "print(\"Saved results to:\", out_path)\n",
    "print(\"Output shape (rows x cols):\", out_df.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "globalrh [~/.conda/envs/globalrh/]",
   "language": "python",
   "name": "conda_globalrh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
